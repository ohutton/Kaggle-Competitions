{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.4.0"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":115578,"databundleVersionId":13895043,"sourceType":"competition"},{"sourceId":937769,"sourceType":"datasetVersion","datasetId":507452},{"sourceId":1483651,"sourceType":"datasetVersion","datasetId":870709},{"sourceId":2876308,"sourceType":"datasetVersion","datasetId":1761664}],"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color:\n#00008B; \n            color:white; \n            padding:15px; \n            border-radius:10px; \n            text-align:center; \n            font-size:36px; \n            font-weight:bold;\">\n    BDA Competition 2: Predicting whether social media posts contain offensive content <br>\n    <span style=\"font-size:20px; font-weight:normal;\">\n        Group 1: Charlotte de Vries, Johannes Degner & Oliver Hutton\n    </span>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#00008b; \n            color:white; \n            padding:12px; \n            border-radius:8px; \n            font-size:24px; \n            font-weight:bold;\">\n    Table of Contents\n</div>\n\n<br>\n\n1. Setup\n\n\n2. Read Data\n\n3. Data Exploration\n\n4. Preprocessing<br>\n   4.1 Tokenization<br>\n   4.2 Stopwords\n\n5. Feature Engineering<br>\n   5.1 Tf-idf and character-level features<br>\n   5.2 Average tweet embeddings<br>\n   5.3 Sentiment analysis<br>\n   4 + 5 Function with preprocessing and features<br>\n   5.4 Merge all features\n\n6. Models<br>\n   6.1 Model fitting<br>\n   6.2 Plotting and picking best set of features<br>\n   6.3 Using best model with more tweets\n\n7. Submission<br>\n   7.1 Converting test tweets to features<br>\n   7.2 Generating predictions for each tweet<br>\n   7.3 Formatting submission file\n\n8. Division of Labour\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#00008B; \n            color:white; \n            padding:12px; \n            border-radius:8px; \n            font-size:30px; \n            font-weight:bold;\">\n    Introduction\n    </div>\n\nThe goal of this competition is to construct a classifier that can accurately recognize and label tweets as including either offensive (1) or non-offensive language (0). \n\nA vast collection of tweets and comments collected from various online platforms provided the data. Human judges assessed the texts to determine whether or not they contained content that was offensive. As a result, the competition's outcomes will generalize to online comments and posts on Twitter and other comparable social media platforms.\n\nThe Ridge-penalized and Lasso-penalized logistic regression models are potential machine learning models to be applied here. To tune the regularization parameter (λ) for the best prediction performance, which is determined primarily by AUC, both models employ cross-validation through the elastic net. \n\nThe feature set derives from tweet text and includes: Lexico-syntactic n-grams (unigram–quadrigram) capturing local word-order cues (negation, modifiers, fixed phrases/slogans), using raw within-tweet counts to preserve intensity/repetition.\nTerm-weighting features—TF, IDF, TF-IDF—to highlight tokens that are locally frequent yet globally distinctive. Surface (structural) features—unique tokens, words, characters, average word length—characterizing lexical diversity, verbosity, and compression/elongation. Semantic embeddings—tweet-level means of pretrained 300-D GloVe vectors—providing compact meaning that groups spelling variants/slang/euphemisms and complements token features on short, noisy text.\nAffective lexicon features—NRC emotion counts (e.g., nrc_anger, nrc_disgust) and a Bing positivity proportion—offering low-dimensional, interpretable tone cues. \nAbusive-language lexicon features—matching a profanity lexicon to uni–quad n-grams (e.g., “fuck you,” “son of a bitch”), capped at 4-grams (only one 5-gram), with tweet-level toxicity scored by summing 1–3 severity ratings across matches.\n\nIt is crucial to recognize that labeling errors of this magnitude are inevitable because some words can be classified as either normal or offensive, depending on the context and possibly the subtle use of tone. Thus, we anticipate an error bound of 5-10%. ","metadata":{"_uuid":"d37b5e33-b7a9-4ad0-a527-25e44b6ad9f5","_cell_guid":"a8db5a8b-75e9-46dd-a98c-b08ece9eb166","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"#Importing packages codeblock\n<div style=\"background-color:#00008B; \n     \t     \tcolor:white; \n        \t\tpadding:12px; \n        \t\tborder-radius:8px; \n         \t  \tfont-size:30px; \n        \t \tfont-weight:bold;\">\n1.Setup\n   \t \t</div>\n\n\nWe start by loading the required packages from tidyverse, and tidytext.","metadata":{}},{"cell_type":"code","source":"## Importing packages\n## test\nlibrary(tidyverse) # metapackage with lots of helpful functions\nlibrary(tidytext)\n\n## Limiting the number of data frame rows displayed\noptions(repr.matrix.max.rows=8)\n\n## Data attached to this notebook\nlist.files(path = \"../input\")","metadata":{"_uuid":"15fc224b-31f4-427b-b074-1f0b2c8e3d32","_cell_guid":"7878080f-90ad-439d-a6f2-e97784976c91","trusted":true,"collapsed":false,"_execution_state":"idle","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-10-08T21:33:24.790771Z","iopub.execute_input":"2025-10-08T21:33:24.793548Z","iopub.status.idle":"2025-10-08T21:33:27.101431Z","shell.execute_reply":"2025-10-08T21:33:27.099473Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#00008B; \n            color:white; \n            padding:12px; \n            border-radius:8px; \n            font-size:30px; \n            font-weight:bold;\">\n    2. Read Data\n</div>\nTo begin with this step, we locate the data and load it into memory.","metadata":{"_uuid":"9fd31ee3-6bde-4b62-8be7-dfaa8527fde1","_cell_guid":"08351b70-00ad-4d27-b384-4b181e070cd8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"dir(\"../input\", recursive=TRUE)","metadata":{"_uuid":"e8831f6b-259a-4cce-aa4e-5c5ce65a1b8c","_cell_guid":"00ee67c0-e512-4fbd-a363-5d4e6e8b265a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-10-08T21:33:27.104397Z","iopub.execute_input":"2025-10-08T21:33:27.148388Z","iopub.status.idle":"2025-10-08T21:33:27.203759Z","shell.execute_reply":"2025-10-08T21:33:27.201433Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The data consist of two separate files. The training data (which is already labeled as either including offensive (1), or not including offensive language (0). The test data is not yet labeled. The training data initially consists of 400,000 tweets, while the test data includes 50,001 tweets. The goal is to build a model on the training data that enables us to accurately predict whether a tweet includes offensive language or not on the test data. ","metadata":{"_uuid":"2c3d3773-92db-4adf-b767-9207616cbdf5","_cell_guid":"284a4ea6-2acf-467b-aa75-9ffd79f3d1b7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Find the right file paths\ntrain_filepath = dir(\"..\", pattern=\"train.csv\", recursive=TRUE, full.names = TRUE)\ntest_filepath = dir(\"..\", pattern=\"test.csv\", recursive=TRUE, full.names = TRUE)\n\n# Read in the csv files\ntraindat = read_csv(train_filepath, col_types=\"cci\") \ntestdat = read_csv(test_filepath, col_types=\"cc\")","metadata":{"_uuid":"5e426773-c0c5-4f12-a6f5-ddcf376b14c3","_cell_guid":"7c16fa83-82b2-4de8-b6d7-2321a84f68ca","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-10-08T21:33:27.207736Z","iopub.execute_input":"2025-10-08T21:33:27.209901Z","iopub.status.idle":"2025-10-08T21:33:29.560524Z","shell.execute_reply":"2025-10-08T21:33:29.558546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"traindat\ntestdat","metadata":{"_uuid":"f0cdc4a1-1a4f-478b-ba6e-3f9c9003c1af","_cell_guid":"632f0ebf-033c-4497-811d-15791e107291","trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:33:29.563464Z","iopub.execute_input":"2025-10-08T21:33:29.565204Z","iopub.status.idle":"2025-10-08T21:33:29.630302Z","shell.execute_reply":"2025-10-08T21:33:29.628404Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#00008B; \n            color:white; \n            padding:12px; \n            border-radius:8px; \n            font-size:30px; \n            font-weight:bold;\">\n    3. Data Exploration\n</div>","metadata":{}},{"cell_type":"markdown","source":"Before creating features and building models, we performed a quick exploratory analysis of our dataset. We compared offensive and normal tweets by looking at their mean number of words, mean number of characters, and mean average word length.","metadata":{}},{"cell_type":"code","source":"# Filter to labelled tweets\neda_dat <- traindat %>%\n  filter(label %in% c(0, 1))\n\n# Character-level features\neda_summary <- eda_dat %>%\n  mutate(\n    word_count   = str_count(tweet, \"\\\\S+\"),\n    char_count   = str_length(tweet),\n    avg_word_len = char_count / pmax(word_count, 1)\n  )\n\n# Summary statistics by label\neda_stats <- eda_summary %>%\n  group_by(label) %>%\n  summarise(\n    mean_words   = mean(word_count, na.rm = TRUE),\n    mean_chars   = mean(char_count, na.rm = TRUE),\n    mean_avg_word_len = mean(avg_word_len, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\neda_stats\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:33:29.633249Z","iopub.execute_input":"2025-10-08T21:33:29.634802Z","iopub.status.idle":"2025-10-08T21:33:32.106918Z","shell.execute_reply":"2025-10-08T21:33:32.104963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set plotting area: 1 row, 3 columns\npar(mfrow = c(1, 3))\n\n# Plot 1: mean words per tweet\nbarplot(\n  height = eda_stats$mean_words,\n  names.arg = eda_stats$label,\n  col = c(\"#00BFC4\", \"#F8766D\"),\n  main = \"Mean Words per Tweet\",\n  xlab = \"Label\",\n  ylab = \"Mean Words\"\n)\n\n# Plot 2: mean characters per tweet\nbarplot(\n  height = eda_stats$mean_chars,\n  names.arg = eda_stats$label,\n  col = c(\"#00BFC4\", \"#F8766D\"),\n  main = \"Mean Characters per Tweet\",\n  xlab = \"Label\",\n  ylab = \"Mean Characters\"\n)\n\n# Plot 3: mean average word length\nbarplot(\n  height = eda_stats$mean_avg_word_len,\n  names.arg = eda_stats$label,\n  col = c(\"#00BFC4\", \"#F8766D\"),\n  main = \"Mean Average Word Length\",\n  xlab = \"Label\",\n  ylab = \"Avg Word Length\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:33:32.109867Z","iopub.execute_input":"2025-10-08T21:33:32.111457Z","iopub.status.idle":"2025-10-08T21:33:32.242503Z","shell.execute_reply":"2025-10-08T21:33:32.240391Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Having plotted our findings, we can see that offensive tweets generally have fewer words and fewer characters. This make sense, considering that offensive statements often rely on shorter and stronger words, for instance \"shut up\" or \"fuck off\". Non-offensive tweets might be more descriptive and convsersational, which inherently has more structure and are longer. However, average word length is similar in both types of tweets.","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#00008B; \n            color:white; \n            padding:12px; \n            border-radius:8px; \n            font-size:30px; \n            font-weight:bold;\">\n    4. Preprocessing\n</div>\n\nOur initial approach was to compute the steps of tokenization, possible stopwords, and feature engineering seperately. However, after using this approach, we concluded that this method was slow and carried the risk of leaking. Therefore, we redesigned preprocessing and feature generation into a single, parameterized feature-builder. A unified function applies the same tokenization, stopword rules, n-gram boundaries, normalizations, and lexicon joins to each split.\n\nThe function eliminates the alignment issues and post-hoc merges by producing a single sparse design matrix with a set column order and distinct, namespaced feature labels (for instance, tfidf:, profanity:, etc...). Runtime and memory use are significantly reduced because tokenization is done just once and is utilised for TF-IDF, n-gram counts, profanity and sentiment matching. Finally, this method enables us to compare models with different sets of features while using fewer tweets (n = 20000), making the process faster than using all tweets. Afterwards, we will select the best model and apply it to more tweets for our final model.","metadata":{"_uuid":"3be8d3ae-c07d-4173-a981-869987873b6b","_cell_guid":"6103ff8b-a170-4a7d-925d-de9694a658bb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 4.1 Tokenization\n\n\nWe first tokenize each tweet. We retain raw within-tweet counts to capture direct lexical signal and intensity (e.g., repeated slurs), giving the model access to uncompromised frequency information before normalization.\n\nWe add bigrams (2-grams) to capture short-range context and compositional meanings that unigrams miss. Especially negation (“not good”), intensifiers/modifiers (“very stupid”, “disgusting behavior”), targeted phrases (“[slur] people”, “illegal immigrants”), and common phrasal verbs/commands (“go back”, “get out”). Raw counts preserve how often such pairings occur within a tweet, reflecting intensity and repetition of phrase-level cues.\n\nTrigrams (3-grams) manage to capture multiword expressions and templated statements typical of tweets (“go back to”, “they are not”), that uni,- and bigrams might miss. They capture stronger intent and polarity than any subset alone. They also help to disambiguate contexts like sarcasm markers (“yeah right sure”) or imperative structures, with counts reflecting repeated use of these patterned constructions.\n\nQuadrigrams (4-grams) enable us to preserve even more. They can capture slogans, threats, and templated incitements that are highly diagnostic but only emerge over multiple tokens. (e.g., “go back to [country]”, “make X great again”, “ban them all now”). Quadrigrams also encode entity-specific frames (group + action + location/time), and raw frequency highlights repeated slogan-level usage within and across tweets.","metadata":{"_uuid":"414f97f0-8d7c-4de1-be09-e61b61e2812b","_cell_guid":"934afeb8-3f51-41d6-8851-437732eb033c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 4.2 Stopwords\n\n\nAt this point of the notebook, making use of stopwords was deemed not appropriate. In some contexts, even words like “you”, “we”, or “them” might be highly informative for detecting offensive language. Therefore, we deliberately retain function words (e.g. pronouns, negation), because – in targeted abuse detection, their distribution can carry crucial cues about intent and target. However, we decided to try a version where we remove stopwords from the word embeddings. Since stopwords carry little semantic meaning, we thought that this might make average tweet embeddings more accurate in predicting new offensive tweets.","metadata":{"_uuid":"7d057a0a-8f0a-4292-8cae-69e1aa4f69b4","_cell_guid":"da28150a-5f6e-491d-90ee-0113782e05f4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"<div style=\"background-color:#00008B; \n            color:white; \n            padding:12px; \n            border-radius:8px; \n            font-size:30px; \n            font-weight:bold;\">\n    5. Feature Engineering\n</div>","metadata":{"_uuid":"a7b35cde-1e6a-425f-b272-4915b5f106db","_cell_guid":"d479dbb3-9b39-4a0c-ac40-7b960a69dbd8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 5.1 Tf-idf and character-level features\n\n***Lexical weighting features***\n\n**Term Frequency (TF)**: We normalize counts by tweet length to prevent longer tweets from dominating purely by verbosity, ensuring each token’s weight reflects its salience within that tweet.\n\t\n**Inverse Document Frequency (IDF)**: It lowers the weight of very common words and boosts rarer, more informative ones, so feature weights better reflect each token’s ability to distinguish tweets across the corpus.\n\n**TF–IDF**: By combining TF and IDF, we emphasize tokens that are both prominent in a tweet and informative globally - an effective, sparse representation for linear classifiers on short texts.\n\n***Lexical diversity***\n\n**Unique tokens per tweet(num_unique_tokens)** We add a tweet-level measure of distinct vocabulary to capture stylistic structure (verbosity vs. repetitiveness), which can correlate with abusive or chant-like discourse beyond word content alone.\n\n***Structural (surface) features***\n\n**Words per tweet**: The number of words per tweet indexes overall verbosity or utterance length; short bursts often mark tags, insults, or slogans, whereas longer tweets typically contain rationale or narrative content.\n\n**Character per tweet**: The total character count provides an orthographic footprint, sensitive to elongations (e.g., “soooo”), emoji or digit density, extended punctuation runs, and highly compressed styles.\n","metadata":{}},{"cell_type":"markdown","source":"### 5.1.1 Non-zero variance features\n\nWe decided to remove tokens that occur in less than 0.01% of the documents. The number 0.01% was chosen arbitrarily, but it should remove idiosyncratic strings and misspellings that occur only in singular tweets. ","metadata":{"_uuid":"4289a362-87ad-4829-9562-b3a821a0d379","_cell_guid":"961b274b-cea0-422d-be34-41ab208a8e84","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 5.2 Average tweet embeddings\n\nWe also include a tweet-level average of pretrained 200-dimensional GloVe word vectors. This gives each tweet a compact “meaning” summary, so similar words—including slang, spelling variants, and euphemisms—are treated alike even when they don’t match exactly. That semantic signal complements TF-IDF’s exact-token focus and is especially helpful because tweets are short and noisy. Using pretrained vectors also brings in knowledge from large text corpora at low cost, improving robustness and generalization.","metadata":{}},{"cell_type":"markdown","source":"## 5.3 Sentiment analysis\nWe load the NRC emotion lexicon and the Bing polarity lexicon, prefix the labels (“nrc_…”, “bing_…”) to avoid name collisions and make feature provenance explicit, and then join them to our tokenized tweets. From NRC we construct per-tweet emotion counts by tallying how many lexicon-matched tokens fall into each category (e.g., nrc_anger, nrc_disgust), resulting in an interpretable affect profile for every tweet. From Bing we compute a frequency-weighted positivity proportion - the share of token counts labeled positive among all lexicon-covered tokens in the tweet - which summarizes overall tone. We include these features because offensive language systematically co-occurs with certain emotions and negative polarity, and these low-dimensional, interpretable signals complement TF–IDF and embeddings by capturing affect rather than exact wording. Although the join excludes out-of-lexicon tokens, this trade-off yields robust cues that improve discrimination on short, noisy tweets.","metadata":{}},{"cell_type":"markdown","source":"Upon inspecting the features, we noticed that certain sentiments also appear as tf-idf features. Hence, we rename the sentiments to start with 'nrc_' for the nrc lexicon, and 'bing_' for the bing lexicon.","metadata":{}},{"cell_type":"markdown","source":"We further decided to use the profanities lexicon and apply it to our tokenized tweets. However, since profanities are not always single words, some of the text in the lexicon consists of multiple words, for instance \"fuck you\" or \"son of a bitch\". We account for this by applying the profanities lexicon to unigrams, bigrams, trigrams and quadgrams. There was only one instance of a slur with 5 words, so we decided to stop at quadgrams. In the profanities lexicon, there is also a severity rating for the insult/slur, ranging from 1 to 3. We decided to use the sum of severity rating of each ngram for each tweet.","metadata":{}},{"cell_type":"markdown","source":"## 4 + 5: Function with preprocessing and feature engineering","metadata":{}},{"cell_type":"code","source":"# Feature builder\nbuild_feature_matrices <- function(n = 15000,\n                                   glove_path = \"/kaggle/input/glove-embeddings/glove.6B.200d.txt\",\n                                   glove_n_lines = 100000,\n                                   ncores = 4) {\n    cat(sprintf(\"\\n[build_feature_matrices] Using n = %d tweets\\n\", n))\n  \n  # Helper: tweet-level character features\n  extract_char_features <- function(data) {\n    data |>\n      mutate(word_count   = str_count(tweet, \"\\\\S+\"),\n             tweet_length = str_length(tweet)) |>\n      select(id, word_count, tweet_length)\n  }\n  \n  ## 3.1 Tokenization of ngrams (uni to quad)\n  tokenized_tweets <- traindat |>\n    slice_head(n = n) |>\n    unnest_tokens(token, tweet, token = \"words\") |>\n    count(id, label, token, name = \"n\")\n  \n  tweets_bigrams <- traindat |>\n    slice_head(n = n) |>\n    unnest_tokens(bigram, tweet, token = \"ngrams\", n = 2) |>\n    filter(!is.na(bigram)) |>\n    count(id, bigram, name = \"n\")\n  \n  tweets_trigrams <- traindat |>\n    slice_head(n = n) |>\n    unnest_tokens(trigram, tweet, token = \"ngrams\", n = 3) |>\n    filter(!is.na(trigram)) |>\n    count(id, trigram, name = \"n\")\n  \n  tweets_quadgrams <- traindat |>\n    slice_head(n = n) |>\n    unnest_tokens(quadgram, tweet, token = \"ngrams\", n = 4) |>\n    filter(!is.na(quadgram)) |>\n    count(id, quadgram, name = \"n\")\n  \n  # token dataset without stopwords\n  tokenized_tweets_no_sw = traindat |>\n    slice_head(n = n) |>\n    unnest_tokens(token, tweet, token = \"words\") |> # tokenize tweets at word level\n    filter(!token %in% stop_words$word) |>\n    count(id, token) # count tokens within tweets as 'n'\n  \n  ## 4.1 TF-IDF + character-level features\n  features <- tokenized_tweets |>\n    bind_tf_idf(token, id, n)\n  \n  tweet_lengths <- tokenized_tweets |>\n    group_by(id) |>\n    summarise(num_unique_tokens = n_distinct(token), .groups = \"drop\")\n  \n  \n  char_feats <- traindat |>\n    slice_head(n = n) |>\n    extract_char_features()\n  \n  features <- features |>\n    left_join(char_feats, by = \"id\") |>\n    left_join(tweet_lengths, by = \"id\")\n  \n  # 4.1.1 Near-zero variance cutoff\n  features_keep <- features |>\n    filter(idf <= -log(0.01 / 100)) |>\n    rename(tweet_id = id)\n  \n  ## 4.2 GloVe embeddings (200d)\n  cat(\"[build_feature_matrices] Reading GloVe…\\n\")\n  embedding_lines <- readLines(glove_path, n = glove_n_lines)\n  \n  emb <- strsplit(embedding_lines, \" \")\n  emb <- do.call(rbind, emb)\n  colnames(emb) <- c(\"token\", paste0(\"dim\", 1:200))\n  glove_tibble <- as_tibble(emb) |> mutate(across(2:201, as.numeric))\n\n  # Join words with GloVe embeddings\n  word_embeddings <- tokenized_tweets |>\n    inner_join(glove_tibble, by = \"token\")\n\n  # Calculate mean of word embedding for each tweet\n  mean_embeddings <- word_embeddings |>\n    group_by(id) |>\n    summarise(across(starts_with(\"dim\"), ~ mean(.x, na.rm = TRUE)), .groups = \"drop\")\n  \n  # Embeddings without stopwords\n  # Join words with GloVe embeddings\n  word_embeddings_nosw <- tokenized_tweets_no_sw %>%\n    inner_join(glove_tibble, by = \"token\")\n  \n  # Calculate mean of word embedding for each tweet\n  mean_embeddings_nosw <- word_embeddings_nosw %>%\n    group_by(id) %>%\n    summarise(across(starts_with(\"dim\"), function(x) mean(x, na.rm = TRUE)))\n  \n  \n  ## 4.3 Sentiment lexicons\n  # Load lexicons\n  nrc  <- read.csv(\"/kaggle/input/bing-nrc-afinn-lexicons/NRC.csv\")\n  bing <- read.csv(\"/kaggle/input/bing-nrc-afinn-lexicons/Bing.csv\")\n\n  # Rename sentiments as they might overlap with the tf-idf word features\n  nrc$sentiment  <- paste0(\"nrc_\", nrc$sentiment)\n  bing$sentiment <- paste0(\"bing_\", bing$sentiment)\n\n  # Calculate sentiment for tweets\n  sentiment_tweets <- tokenized_tweets |>\n    inner_join(nrc,\n               by = c(\"token\" = \"word\"),\n               relationship = \"many-to-many\") |>\n    count(id, sentiment, name = \"n\") |>\n    pivot_wider(\n      id_cols = id,\n      names_from = sentiment,\n      values_from = n,\n      values_fill = 0\n    )\n                     \n  # Calcualte average negativity per tweet\n  avg_negativity <- tokenized_tweets |>\n    inner_join(bing,\n               by = c(\"token\" = \"word\"),\n               relationship = \"many-to-many\") |>\n    mutate(is_negative = ifelse(sentiment == \"bing_negative\", 1, 0)) |>\n    group_by(id) |>\n    summarise(negativity = sum(is_negative * n) / sum(n),\n              .groups = \"drop\")\n  \n  ## Profanities, looking at all ngrams\n  profanities <- read.csv(\"/kaggle/input/profanities-in-english-collection/profanity_en.csv\")\n  profanities_clean <- profanities |>\n    mutate(text = str_trim(str_to_lower(text)))\n                     \n  profanity_unigrams   <- profanities_clean |> filter(!str_detect(text, \" \"))\n  profanity_bigrams    <- profanities_clean |> filter(str_count(text, \" \") == 1)\n  profanity_trigrams   <- profanities_clean |> filter(str_count(text, \" \") == 2)\n  profanity_quadgrams  <- profanities_clean |> filter(str_count(text, \" \") == 3)\n                     \n  # Profanity rating for unigrams in tweets and sum the rating for each tweet\n  profanity_rating_unigrams <- tokenized_tweets |>\n    left_join(\n      profanity_unigrams |> select(text, severity_rating),\n      by = c(\"token\" = \"text\"),\n      relationship = \"many-to-many\"\n    ) |>\n    mutate(across(everything(), ~ replace_na(.x, 0))) |>\n    group_by(id) |>\n    summarise(severity_unigram = sum(severity_rating),\n              .groups = \"drop\")\n                     \n  # Profanity rating for bigrams in tweets and sum the rating for each tweet\n  profanity_rating_bigrams <- tweets_bigrams |>\n    left_join(\n      profanity_bigrams |> select(text, severity_rating),\n      by = c(\"bigram\" = \"text\"),\n      relationship = \"many-to-many\"\n    ) |>\n    mutate(across(everything(), ~ replace_na(.x, 0))) |>\n    group_by(id) |>\n    summarise(severity_bigram = sum(severity_rating),\n              .groups = \"drop\")\n                     \n  # Profanity rating for trigrams in tweets and sum the rating for each tweet\n  profanity_rating_trigrams <- tweets_trigrams |>\n    left_join(\n      profanity_trigrams |> select(text, severity_rating),\n      by = c(\"trigram\" = \"text\"),\n      relationship = \"many-to-many\"\n    ) |>\n    mutate(across(everything(), ~ replace_na(.x, 0))) |>\n    group_by(id) |>\n    summarise(severity_trigram = sum(severity_rating),\n              .groups = \"drop\")\n                     \n  # Profanity rating for quad in tweets and sum the rating for each tweet\n  profanity_rating_quadgrams <- tweets_quadgrams |>\n    left_join(\n      profanity_quadgrams |> select(text, severity_rating),\n      by = c(\"quadgram\" = \"text\"),\n      relationship = \"many-to-many\"\n    ) |>\n    mutate(across(everything(), ~ replace_na(.x, 0))) |>\n    group_by(id) |>\n    summarise(severity_quadgram = sum(severity_rating),\n              .groups = \"drop\")\n                     \n  # join all ratings together\n  profanity_total_rating <- profanity_rating_unigrams |>\n    left_join(profanity_rating_bigrams, by = \"id\") |>\n    left_join(profanity_rating_trigrams, by = \"id\") |>\n    left_join(profanity_rating_quadgrams, by = \"id\") |>\n    mutate(across(starts_with(\"severity\"), ~ replace_na(.x, 0)))\n                     \n  # and sum them together\n  profanity_sum <- profanity_total_rating |>\n    transmute(id,\n              profanity_sum_rating =\n                severity_unigram + severity_bigram + severity_trigram + severity_quadgram)\n  \n  \n  ## Align IDs and build sparse matrices\n  all_ids <- sort(unique(features_keep$tweet_id))\n  \n  # tf-idf\n  X_tfidf <- features_keep |>\n    mutate(tweet_id = factor(tweet_id, levels = all_ids)) |>\n    cast_sparse(tweet_id, token, tf_idf)\n  \n  # character features\n  X_chars <- features_keep |>\n    select(tweet_id, tweet_length, word_count, num_unique_tokens) |>\n    pivot_longer(-tweet_id, names_to = \"feature\", values_to = \"value\") |>\n    mutate(tweet_id = factor(tweet_id, levels = all_ids)) |>\n    cast_sparse(tweet_id, feature, value)\n  \n  # embeddings\n  X_emb <- mean_embeddings |>\n    \n    filter(id %in% all_ids) |>\n    mutate(tweet_id = factor(id, levels = all_ids)) |>\n    pivot_longer(starts_with(\"dim\"),\n                 names_to = \"feature\",\n                 values_to = \"value\") |>\n    cast_sparse(tweet_id, feature, value)\n\n  # embeddings without stopwords                   \n  X_emb_nosw <- mean_embeddings_nosw %>% # without stopwords\n    filter(id %in% all_ids) %>%\n    mutate(tweet_id = factor(id, levels = all_ids)) %>%\n    pivot_longer(cols = starts_with(\"dim\"),\n                 names_to = \"feature\",\n                 values_to = \"value\") %>%\n    cast_sparse(tweet_id, feature, value)\n  \n  # NRC, ensuring full row coverage\n  valid_ids_nrc <- intersect(all_ids, sentiment_tweets$id)\n  X_nrc_tmp <- sentiment_tweets |>\n    filter(id %in% valid_ids_nrc) |>\n    rename(tweet_id = id) |>\n    mutate(tweet_id = factor(tweet_id, levels = valid_ids_nrc)) |>\n    pivot_longer(-tweet_id, names_to = \"sentiment_nrc\", values_to\n                 = \"count_nrc\") |>\n    cast_sparse(tweet_id, sentiment_nrc, count_nrc)\n\n  # when running it the first time, there were some missing row ids\n  # so we account for this now by filling them to 0              \n  missing_ids_nrc <- setdiff(all_ids, rownames(X_nrc_tmp))\n  if (length(missing_ids_nrc) > 0) {\n    empty_mat <- Matrix::sparseMatrix(\n      i = integer(0),\n      j = integer(0),\n      x = numeric(0),\n      dims = c(length(missing_ids_nrc), ncol(X_nrc_tmp)),\n      dimnames = list(missing_ids_nrc, colnames(X_nrc_tmp))\n    )\n    X_nrc <- rbind(X_nrc_tmp, empty_mat)\n    X_nrc <- X_nrc[all_ids, ]\n  } else {\n    X_nrc <- X_nrc_tmp[all_ids, ]\n  }\n  \n  # Bing, counts per sentiment\n  valid_ids_bing <- intersect(all_ids, sentiment_tweets$id)\n  X_bing_tmp <- sentiment_tweets |>\n    filter(id %in% valid_ids_bing) |>\n    rename(tweet_id = id) |>\n    \n    mutate(tweet_id = factor(tweet_id, levels = valid_ids_bing)) |>\n    pivot_longer(-tweet_id, names_to = \"sentiment_bing\", values_to = \"count_bing\") |>\n    cast_sparse(tweet_id, sentiment_bing, count_bing)\n\n  # same as for nrc, there were some missing row ids\n  # we account for this now by filling them with 0\n  missing_ids_bing <- setdiff(all_ids, rownames(X_bing_tmp))\n  if (length(missing_ids_bing) > 0) {\n    empty_mat <- Matrix::sparseMatrix(\n      i = integer(0),\n      j = integer(0),\n      x = numeric(0),\n      dims = c(length(missing_ids_bing), ncol(X_bing_tmp)),\n      dimnames = list(missing_ids_bing, colnames(X_bing_tmp))\n    )\n    X_bing <- rbind(X_bing_tmp, empty_mat)\n    X_bing <- X_bing[all_ids, ]\n  } else {\n    X_bing <- X_bing_tmp[all_ids, ]\n  }\n  \n  # Profanity score\n  X_profanities <- profanity_sum |>\n    filter(id %in% all_ids) |>\n    mutate(tweet_id = factor(id, levels = all_ids)) |>\n    transmute(tweet_id, feature = \"profanity_rating\", value = profanity_sum_rating) |>\n    cast_sparse(tweet_id, feature, value)\n  \n  \n  list(\n    ids = all_ids,\n    X_tfidf = X_tfidf,\n    X_chars = X_chars,\n    X_emb = X_emb,\n    X_emb_nosw = X_emb_nosw,\n    X_nrc = X_nrc,\n    X_bing = X_bing,\n    X_profanities = X_profanities\n  )\n  \n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:33:32.245736Z","iopub.execute_input":"2025-10-08T21:33:32.247599Z","iopub.status.idle":"2025-10-08T21:33:32.265676Z","shell.execute_reply":"2025-10-08T21:33:32.263751Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.4 Merge all features","metadata":{}},{"cell_type":"markdown","source":"Here, we merge all the feature sets into 5 different versions:\n- V1: Tf-idf + Character-level features\n- V2: Tf-idf + Character-level features + Sentiment analyses\n- V3: Tf-idf + Character-level features + Sentiment analyses + average tweet embeddings\n- V4: Character-level features + Sentiment analyses + average tweet embeddings\n- V5: Tf-idf + Character-level features + Sentiment analyses + average tweet embeddings no stopwords","metadata":{}},{"cell_type":"code","source":"# Initial run with 20000\nbuilt <- build_feature_matrices(n = 20000)\n\nX_v1 <- cbind(built$X_tfidf, built$X_chars)\nX_v2 <- cbind(built$X_tfidf, built$X_chars, built$X_nrc, built$X_profanities, built$X_bing)\nX_v3 <- cbind(built$X_tfidf, built$X_chars, built$X_nrc, built$X_profanities, built$X_bing, built$X_emb)\nX_v4 <- cbind(built$X_nrc, built$X_profanities, built$X_bing, built$X_emb)\nX_v5 <- cbind(built$X_tfidf, built$X_chars, built$X_nrc, built$X_profanities, built$X_bing, built$X_emb_nosw)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T07:58:17.012139Z","iopub.execute_input":"2025-10-09T07:58:17.018478Z","iopub.status.idle":"2025-10-09T07:58:17.221451Z","shell.execute_reply":"2025-10-09T07:58:17.218742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#00008B; \n            color:white; \n            padding:12px; \n            border-radius:8px; \n            font-size:30px; \n            font-weight:bold;\">\n    6. Models\n</div>","metadata":{"_uuid":"70296bd4-e07a-49c9-b071-a5da80206e89","_cell_guid":"16e82cc2-2c2a-4d90-a5d1-0ffeebef1922","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## 6.1 Model fitting","metadata":{"_uuid":"312a8cd7-f29a-480c-a025-8888567df8ab","_cell_guid":"3b97c23f-3ea3-47d0-b6a0-13dada0af064","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"The code below creates the target vector `y`. \nWe ensure that every row of `X` has the correct corresponding entry in `y`. \nWe use the training–validation set approach for selecting between models; due to the high dimensional data, this is faster than doing cross-validation.\n\nWe decided to use elastic net as it performs particularly well with high-dimensional data, and allows for flexibility in regularisation by tuning the hyperparameter. When alpha = 1, only the L1 penalty is applied, which is a lasso regression. When alpha = 0, only the L2 penalty is applied, which is a ridge regression. Elastic net has the advantage that it can combine the weights of the two penalties by setting alpha to different values. Therefore, we opted to use a range of values for alpha (0, 0.2, 0.4, 0.6, 0.8 and 1) and create a function for this.","metadata":{"_uuid":"be804b43-7a83-459c-ac28-8c08fd3c0699","_cell_guid":"1032f6c6-f7d3-4287-a584-b44d0c355646","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Elastic net evaluator\nevaluate_elastic_net_model <- function(X, alpha_values = seq(0, 1, 0.2)) {\n  set.seed(2025)\n\n  # Build y from X rownames joined to traindat\n  y <- data.frame(id = rownames(X)) |>\n    inner_join(traindat, by = \"id\") |>\n    pull(label)\n\n  # Train/valid split\n  trainidx <- caret::createDataPartition(y, p = 0.8)$Resample1\n  Xtrain <- X[trainidx, ]\n  ytrain <- y[trainidx]\n  Xvalid <- X[-trainidx, ]\n  yvalid <- y[-trainidx]\n\n  # Parallel folds\n  doMC::registerDoMC(cores = 4)\n\n  results <- tibble(alpha = numeric(), auc = numeric(), lambda_min = numeric())\n  best_auc   <- -Inf\n  best_model <- NULL\n  best_alpha <- NA_real_\n\n  for (a in alpha_values) {\n    fit <- glmnet::cv.glmnet(\n      Xtrain, ytrain,\n      alpha = a,\n      nfolds = 3,\n\n      family = \"binomial\",\n      type.measure = \"auc\",\n      parallel = TRUE\n    )\n    preds <- predict(fit, Xvalid, s = \"lambda.min\", type = \"response\")\n    auc_v <- glmnet::assess.glmnet(preds, newy = yvalid, family = \"binomial\")$auc\n\n    results <- add_row(results, alpha = a, auc = as.numeric(auc_v), lambda_min = fit$lambda.min)\n\n    if (auc_v > best_auc) {\n      best_auc   <- auc_v\n      best_model <- fit\n      best_alpha <- a\n    }\n  }\n\n  list(results = results, best_model = best_model, best_alpha = best_alpha)\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:34:21.897312Z","iopub.execute_input":"2025-10-08T21:34:21.899067Z","iopub.status.idle":"2025-10-08T21:34:21.915377Z","shell.execute_reply":"2025-10-08T21:34:21.913512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run function on each set of features\nresults_v1 <- evaluate_elastic_net_model(X_v1)\nresults_v2 <- evaluate_elastic_net_model(X_v2)\nresults_v3 <- evaluate_elastic_net_model(X_v3)\nresults_v4 <- evaluate_elastic_net_model(X_v4)\nresults_v5 <- evaluate_elastic_net_model(X_v5)\n\n# Label each auc score with the appropriate version\nresults_v1$results$features <- \"V1\"\nresults_v2$results$features <- \"V2\"\nresults_v3$results$features <- \"V3\"\nresults_v4$results$features <- \"V4\"\nresults_v5$results$features <- \"V5\"\n\n# Combine all results\nresults_all <- dplyr::bind_rows(\n  results_v1$results,\n  results_v2$results,\n  results_v3$results,\n  results_v4$results,\n  results_v5$results\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:34:21.919150Z","iopub.execute_input":"2025-10-08T21:34:21.920740Z","iopub.status.idle":"2025-10-08T21:35:12.266751Z","shell.execute_reply":"2025-10-08T21:35:12.264152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.2 Plotting and picking best set of features\n\nTo compare and help visualise the different models with the various sets of features, we decided to plot each model.","metadata":{}},{"cell_type":"code","source":"ggplot(results_all, aes(x = alpha, y = auc, color = features)) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0, 1, 0.2)) +\n  labs(\n    title = \"Elastic Net AUC by Feature Set and Alpha\",\n    x = \"Alpha (Elastic Net Mixing Parameter)\",\n    y = \"AUC\",\n    color = \"Feature Set\"\n  ) +\n  theme_minimal(base_size = 14)\n\n# Pick the overall winner across V1 to V5\npick_best <- function() {\n  sets <- list(\n    V1 = results_v1,\n    V2 = results_v2,\n    V3 = results_v3,\n    V4 = results_v4,\n    V5 = results_v5\n  )\n  best <- tibble(\n    set   = names(sets),\n    auc   = sapply(sets, function(x) max(x$results$auc, na.rm = TRUE)),\n    alpha = sapply(sets, function(x) x$best_alpha)\n  )\n  best <- arrange(best, desc(auc)) %>% slice(1)\n  best\n}\n\nbest_small <- pick_best()\nprint(best_small)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:35:12.270534Z","iopub.execute_input":"2025-10-08T21:35:12.272685Z","iopub.status.idle":"2025-10-08T21:35:13.049510Z","shell.execute_reply":"2025-10-08T21:35:13.046363Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Summary of different versions\n- V1: Tf-idf + Character features\n- V2: Tf-idf + Character features + Sentiment analyses\n- V3: Tf-idf + Character features + Sentiment analyses + average tweet embeddings\n- V4: Character features + Sentiment analyses + average tweet embeddings\n- V5: Tf-idf + Character features + Sentiment analyses + average tweet embeddings no stopwords\n\nThe best model will vary depending on how many tweets we use. When more tweets are used, we expect the models including average tweet embeddings to perform better.","metadata":{}},{"cell_type":"markdown","source":"## 6.3 Using best model with more tweets","metadata":{}},{"cell_type":"markdown","source":"Now that we have a best model, we will re-train a new model with the same features but with more tweets. Here, we decided to run the model again using 170'000 tweets - a third of the dataset. This makes it less computationally intense than using the entire dataset, but we still include enough data to make accurate predictions.","metadata":{}},{"cell_type":"code","source":"\n## more tweets with same features as winner\nbuilt_new <- build_feature_matrices(n = 130000)\n\n# Here, we used an LLM to help us construct the best-performing model \n## ==> LLM Start src: https://chatgpt.com/s/t_68e75a75a9588191a3aba5c4bf723f9f\n# Helper to construct the winning X on demand\nbuild_X_from_set <- function(set_name, built_obj) {\n  switch(set_name,\n    V1 = cbind(built_obj$X_tfidf, built_obj$X_chars),\n    V2 = cbind(built_obj$X_tfidf, built_obj$X_chars, built_obj$X_nrc, built_obj$X_profanities, built_obj$X_bing),\n    V3 = cbind(built_obj$X_tfidf, built_obj$X_chars, built_obj$X_nrc, built_obj$X_profanities, built_obj$X_bing, built_obj$X_emb),\n    V4 = cbind(built_obj$X_nrc, built_obj$X_profanities, built_obj$X_bing, built_obj$X_emb),\n    V5 = cbind(built_obj$X_tfidf, built_obj$X_chars, built_obj$X_nrc, built_obj$X_profanities, built_obj$X_bing, built_obj$X_emb_nosw),\n    stop(\"Unknown feature set: \", set_name)\n  )\n}\n\n\nX_best <- build_X_from_set(best_small$set, built_new)\n## ==> LLM End\n\n# Get y aligned to the matrix rows\ny_new <- data.frame(id = rownames(X_best)) %>%\n  inner_join(traindat, by = \"id\") %>%\n  pull(label)\n\n# Train final model on more tweets with the same alpha as the best model from before\ndoMC::registerDoMC(cores = 4)\nalpha_star <- best_small$alpha\n\ncat(\"Refitting with more tweets with set =\", best_small$set, \" alpha =\", alpha_star, \"\\n\")\n\nfinal_fit <- glmnet::cv.glmnet(\n  x = X_best,\n  y = y_new,\n  alpha = alpha_star,\n  nfolds = 3,\n  family = \"binomial\",\n  type.measure = \"auc\",\n  parallel = TRUE\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:35:13.054526Z","iopub.execute_input":"2025-10-08T21:35:13.057024Z","iopub.status.idle":"2025-10-08T21:36:08.930899Z","shell.execute_reply":"2025-10-08T21:36:08.928467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# make predictions\nprob_preds <- predict(final_fit, newx = X_best, s = \"lambda.min\", type = \"response\")\n\n# ROC\nroc_embedding <- glmnet::roc.glmnet(as.numeric(prob_preds), newy = y_new)\n\n# plotting the AUC\nplot(roc_embedding,type='l', main=\"Receiver Operating Characteristic function\")\n\n# computing the AUC\nglmnet::assess.glmnet(prob_preds, newy = y_new, family=\"binomial\")$auc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:36:08.935225Z","iopub.execute_input":"2025-10-08T21:36:08.936964Z","iopub.status.idle":"2025-10-08T21:36:09.223769Z","shell.execute_reply":"2025-10-08T21:36:09.220478Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<div style=\"background-color:#00008B; \n            color:white; \n            padding:12px; \n            border-radius:8px; \n            font-size:30px; \n            font-weight:bold;\">\n    7. Submitting Predictions\n</div>","metadata":{"_uuid":"9fac914f-79fa-41da-9335-4cde01a0f57f","_cell_guid":"d47592ee-c97f-4426-ab11-c333eb11826b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"sample_filepath = dir(\"..\", pattern=\"sample.*.csv\", recursive=TRUE, full.names = TRUE)\n\nsample_submission = read_csv(sample_filepath, col_types = cols(col_character(), col_double()))\n\nhead(sample_submission)","metadata":{"_uuid":"75fb6f45-d4b0-4dfd-9bd1-f78ade1cc273","_cell_guid":"c971527d-4fa3-41dd-bdbf-c2fc9b47dad2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-10-08T22:31:54.839572Z","iopub.execute_input":"2025-10-08T22:31:54.847041Z","iopub.status.idle":"2025-10-08T22:31:55.025445Z","shell.execute_reply":"2025-10-08T22:31:55.023080Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7.1 Converting test tweets to features\n\nWe have to create a sparse matrix for each set of features (tf-idf, mean tweet embeddings, and sentiment analysis with NRC and Bing), and combine the matrices.\n","metadata":{"_uuid":"c2ca7086-c006-4b52-95e5-1e94e1319cc6","_cell_guid":"e2bdabc1-2699-41bf-9089-89179f7e727d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"## Some helper functions\n\n# Create the design matrix from built components\nbuild_X_from_set <- function(set_name, b) {\n  switch(set_name,\n    V1 = cbind(b$X_tfidf, b$X_chars),\n    V2 = cbind(b$X_tfidf, b$X_chars, b$X_nrc, b$X_profanities, b$X_bing),\n    V3 = cbind(b$X_tfidf, b$X_chars, b$X_nrc, b$X_profanities, b$X_bing, b$X_emb),\n    V4 = cbind(b$X_nrc, b$X_profanities, b$X_bing, b$X_emb),\n    V5 = cbind(b$X_tfidf, b$X_chars, b$X_nrc, b$X_profanities, b$X_bing, b$X_emb_nosw),\n    stop(\"Unknown feature set: \", set_name)\n  )\n}\n\n# Align test sparse matrix columns to training matrix columns\nalign_sparse_cols <- function(M, ref_cols) {\n  cur  <- colnames(M)\n  miss <- setdiff(ref_cols, cur)\n  if (length(miss) > 0) {\n    zero_block <- Matrix::sparseMatrix(\n      i = integer(0), j = integer(0), x = numeric(0),\n      dims = c(nrow(M), length(miss)),\n      dimnames = list(rownames(M), miss)\n    )\n    M <- cbind(M, zero_block)\n  }\n  M[, ref_cols, drop = FALSE]\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:36:09.334376Z","iopub.execute_input":"2025-10-08T21:36:09.336264Z","iopub.status.idle":"2025-10-08T21:36:09.355361Z","shell.execute_reply":"2025-10-08T21:36:09.353395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Training idf\n# use the same n that we trained on (130000)\nn_train_idf <- 130000\n\ntrain_tokens_for_idf <- traindat %>%\n  slice_head(n = n_train_idf) %>%\n  mutate(id = as.character(id)) %>%\n  unnest_tokens(token, tweet, token = \"words\") %>%\n  count(id, token, name = \"n\")\n\nidf_map <- train_tokens_for_idf %>%\n  bind_tf_idf(token, id, n) %>%\n  distinct(token, idf)\n\nn_docs_train <- n_distinct(train_tokens_for_idf$id)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:36:09.359587Z","iopub.execute_input":"2025-10-08T21:36:09.361669Z","iopub.status.idle":"2025-10-08T21:36:10.006045Z","shell.execute_reply":"2025-10-08T21:36:10.003996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Basic objects\ntest_small <- testdat %>% mutate(id = as.character(id))\ntest_ids   <- test_small$id\n\n# Character features\ntest_char_feats <- test_small %>%\n  transmute(\n    id,\n    word_count   = str_count(tweet, \"\\\\S+\"),\n    tweet_length = str_length(tweet)\n  )\n\n# Unigram tokens for test\ntest_tokens <- test_small %>%\n  unnest_tokens(token, tweet, token = \"words\") %>%\n  count(id, token, name = \"n\")\n\n# Unique-token counts (for X_chars)\ntest_num_unique <- test_tokens %>%\n  group_by(id) %>%\n  summarise(num_unique_tokens = n_distinct(token), .groups = \"drop\")\n\n# tf-idf on test using training idf\nsum_n <- test_tokens %>% group_by(id) %>% summarise(sum_n = sum(n), .groups = \"drop\")\ntest_tf <- test_tokens %>%\n  left_join(sum_n, by = \"id\") %>%\n  mutate(tf = n / pmax(sum_n, 1))\n\ntest_tfidf <- test_tf %>%\n  left_join(idf_map, by = \"token\") %>%\n  mutate(idf = ifelse(is.na(idf), log(n_docs_train), idf),\n         tf_idf = tf * idf) %>%\n  transmute(tweet_id = id, token, tf_idf)\n\n# convert to sparse matrices\nX_tfidf_test <- test_tfidf %>%\n  cast_sparse(tweet_id, token, tf_idf)\n\nX_chars_test <- test_char_feats %>%\n  left_join(test_num_unique, by = \"id\") %>%\n  rename(tweet_id = id) %>%\n  pivot_longer(-tweet_id, names_to = \"feature\", values_to = \"value\") %>%\n  cast_sparse(tweet_id, feature, value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:36:10.010876Z","iopub.execute_input":"2025-10-08T21:36:10.012711Z","iopub.status.idle":"2025-10-08T21:36:36.419478Z","shell.execute_reply":"2025-10-08T21:36:36.417459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Do the same for the sentiment analysis\nnrc <- read.csv(\"/kaggle/input/bing-nrc-afinn-lexicons/NRC.csv\")\nbing <- read.csv(\"/kaggle/input/bing-nrc-afinn-lexicons/Bing.csv\")\nnrc$sentiment <- paste0(\"nrc_\",  nrc$sentiment)\nbing$sentiment <- paste0(\"bing_\", bing$sentiment)\n\n# nrc counts\nnrc_counts_test <- test_tokens %>%\n  inner_join(nrc, by = c(\"token\" = \"word\"), relationship = \"many-to-many\") %>%\n  count(id, sentiment, wt = n, name = \"cnt\")\n\nX_nrc_test <- nrc_counts_test %>%\n  transmute(tweet_id = id, sentiment_nrc = sentiment, count_nrc = cnt) %>%\n  cast_sparse(tweet_id, sentiment_nrc, count_nrc)\n\n# bing counts\nbing_counts_test <- test_tokens %>%\n  inner_join(bing, by = c(\"token\" = \"word\"), relationship = \"many-to-many\") %>%\n  count(id, sentiment, wt = n, name = \"cnt\")\n\n#convert to sparse matrix\nX_bing_test <- bing_counts_test %>%\n  transmute(tweet_id = id, sentiment_bing = sentiment, count_bing = cnt) %>%\n  cast_sparse(tweet_id, sentiment_bing, count_bing)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:36:36.424076Z","iopub.execute_input":"2025-10-08T21:36:36.425823Z","iopub.status.idle":"2025-10-08T21:36:41.507321Z","shell.execute_reply":"2025-10-08T21:36:41.505107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Do the same for the profanities lexicon\nprofanities <- read.csv(\"/kaggle/input/profanities-in-english-collection/profanity_en.csv\") %>%\n  mutate(text = str_trim(str_to_lower(text)))\nprof_uni <- profanities %>% filter(!str_detect(text, \" \"))\n\nprofanity_scores_test <- test_tokens %>%\n  left_join(prof_uni %>% select(text, severity_rating),\n            by = c(\"token\" = \"text\"), relationship = \"many-to-many\") %>%\n  mutate(severity_rating = replace_na(severity_rating, 0)) %>%\n  group_by(id) %>%\n  summarise(profanity_rating = sum(n * severity_rating), .groups = \"drop\")\n\n#convert to sparse matrix\nX_profanities_test <- profanity_scores_test %>%\n  transmute(tweet_id = id, feature = \"profanity_rating\", value = profanity_rating) %>%\n  cast_sparse(tweet_id, feature, value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:36:41.512195Z","iopub.execute_input":"2025-10-08T21:36:41.514038Z","iopub.status.idle":"2025-10-08T21:36:42.147474Z","shell.execute_reply":"2025-10-08T21:36:42.145487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read GloVe once (same file & dims as training)\nembedding_lines <- readLines(\"/kaggle/input/glove-embeddings/glove.6B.200d.txt\", n = 100000)\nemb <- strsplit(embedding_lines, \" \")\nemb <- do.call(rbind, emb)\ncolnames(emb) <- c(\"token\", paste0(\"dim\", 1:200))\nglove_tibble <- as_tibble(emb) %>% mutate(across(2:201, as.numeric))\n\n# With all tokens\nword_embeddings_test <- test_tokens %>% inner_join(glove_tibble, by = \"token\")\nmean_embeddings_test <- word_embeddings_test %>%\n    group_by(id) %>%\n    summarise(across(starts_with(\"dim\"), ~mean(.x, na.rm = TRUE)), .groups = \"drop\")\n\nX_emb_test <- mean_embeddings_test %>%\n    mutate(tweet_id = id) %>% select(-id) %>%\n    pivot_longer(-tweet_id, names_to = \"feature\", values_to = \"value\") %>%\n    cast_sparse(tweet_id, feature, value)\n    \n# Without stopwords\n# Build X_emb_nosw_test only if V5 is the best model, computationally less intense\nif (best_small$set == \"V5\") {\n  test_tokens_nosw <- test_small %>%\n    unnest_tokens(token, tweet, token = \"words\") %>%\n    anti_join(stop_words, by = c(\"token\" = \"word\")) %>%\n    count(id, token, name = \"n\")\n\n  # re-use glove_tibble we loaded for embeddings\n  word_embeddings_nosw_test <- test_tokens_nosw %>% inner_join(glove_tibble, by = \"token\")\n  mean_embeddings_nosw_test <- word_embeddings_nosw_test %>%\n    group_by(id) %>%\n    summarise(across(starts_with(\"dim\"), ~mean(.x, na.rm = TRUE)), .groups = \"drop\")\n\n  X_emb_nosw_test <- mean_embeddings_nosw_test %>%\n    mutate(tweet_id = id) %>% select(-id) %>%\n    pivot_longer(-tweet_id, names_to = \"feature\", values_to = \"value\") %>%\n    cast_sparse(tweet_id, feature, value)\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:36:42.152084Z","iopub.execute_input":"2025-10-08T21:36:42.153995Z","iopub.status.idle":"2025-10-08T21:40:34.192791Z","shell.execute_reply":"2025-10-08T21:40:34.190777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decide the row order we want\ntest_ids <- test_small$id\n\n## ==> LLM Start src: https://chatgpt.com/s/t_68e75a75a9588191a3aba5c4bf723f9f\n# Helper: pad to all rows in the right order\npad_rows <- function(M, ids) {\n  if (is.null(M)) return(NULL)\n  rn <- rownames(M)\n  miss <- setdiff(ids, rn)\n  if (length(miss) > 0) {\n    empty <- Matrix::sparseMatrix(i = integer(0), j = integer(0), x = numeric(0),\n                                  dims = c(length(miss), ncol(M)),\n                                  dimnames = list(miss, colnames(M)))\n    M <- rbind(M, empty)\n  }\n  M[ids, , drop = FALSE]\n}\n## ==> LLM End\n\n# Which components are needed for a given set\nneeded_for <- function(set_name) {\n  switch(set_name,\n    V1 = c(\"tfidf\",\"chars\"),\n    V2 = c(\"tfidf\",\"chars\",\"nrc\",\"profanities\",\"bing\"),\n    V3 = c(\"tfidf\",\"chars\",\"nrc\",\"profanities\",\"bing\",\"emb\"),\n    V4 = c(\"nrc\",\"profanities\",\"bing\",\"emb\"),\n    V5 = c(\"tfidf\",\"chars\",\"nrc\",\"profanities\",\"bing\",\"emb_nosw\")\n  )\n}\n\n# Build the list of components our set actually needs\nneed <- needed_for(best_small$set)\n\n# using helper funciton, fill missing rows\nbuilt_test <- list(ids = test_ids)\nif (\"tfidf\" %in% need) built_test$X_tfidf <- pad_rows(X_tfidf_test, test_ids)\nif (\"chars\" %in% need) built_test$X_chars <- pad_rows(X_chars_test, test_ids)\nif (\"nrc\" %in% need) built_test$X_nrc <- pad_rows(X_nrc_test, test_ids)\nif (\"bing\" %in% need) built_test$X_bing <- pad_rows(X_bing_test, test_ids)\nif (\"profanities\" %in% need) built_test$X_profanities <- pad_rows(X_profanities_test, test_ids)\nif (\"emb\" %in% need) built_test$X_emb <- pad_rows(X_emb_test, test_ids)\nif (\"emb_nosw\" %in% need) built_test$X_emb_nosw <- pad_rows(X_emb_nosw_test, test_ids)\n\n# Bind in the right order for your winning set\nX_test <- build_X_from_set(best_small$set, built_test)\n\n# Align columns to the exact training matrix we used to fit final_fit\nX_test_aligned <- align_sparse_cols(X_test, colnames(X_best))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:40:34.196513Z","iopub.execute_input":"2025-10-08T21:40:34.198222Z","iopub.status.idle":"2025-10-08T21:40:36.883604Z","shell.execute_reply":"2025-10-08T21:40:36.881633Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### 7.2 Generating predictions for each tweet\n\nNext we employ our final model to get probabilities for each tweet to contain offensive language:","metadata":{"_uuid":"5da65d1e-142b-4e53-ab55-fbd0d7449991","_cell_guid":"9d969260-d24e-49b6-9532-64333f147ddc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Predict probabilities with the trained model\npred_prob_final <- as.numeric(predict(final_fit, newx = X_test_aligned, s = \"lambda.min\", type = \"response\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T21:40:36.886557Z","iopub.execute_input":"2025-10-08T21:40:36.888112Z","iopub.status.idle":"2025-10-08T21:40:37.056219Z","shell.execute_reply":"2025-10-08T21:40:37.054190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dim(testdat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T22:37:52.622306Z","iopub.execute_input":"2025-10-08T22:37:52.624405Z","iopub.status.idle":"2025-10-08T22:37:52.658870Z","shell.execute_reply":"2025-10-08T22:37:52.653880Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### 7.3 Writing the submission file","metadata":{"_uuid":"c6402dfc-31c3-4dc4-8749-a6efd15e6c4d","_cell_guid":"f0602562-9a38-427e-a603-20a58abdeb78","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# made it 2D instead of a vector\npred_prob_final <- cbind(prob = as.numeric(pred_prob_final))\nrownames(pred_prob_final) <- rownames(X_test_aligned)\n\ntibble(id = rownames(pred_prob_final), prob = pred_prob_final[,1]) |>\n    right_join(testdat, by=\"id\") |>\n    arrange(as.numeric(id)) |> \n    replace_na(list(prob = mean(y_new))) |>\n    select(-tweet) |>\n    write_csv(\"submission.csv\")\n\n# Check to see if the format is correct\nlist.files()\nread_csv(\"submission.csv\", col_types=\"cd\") # should be 50,001 x 2","metadata":{"_uuid":"ed2447fa-25e1-426a-9a3f-c283c542652b","_cell_guid":"4ceb06c3-f1b3-4bce-85f6-18243835458c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-10-08T22:42:02.019236Z","iopub.execute_input":"2025-10-08T22:42:02.022539Z","iopub.status.idle":"2025-10-08T22:42:02.353441Z","shell.execute_reply":"2025-10-08T22:42:02.349782Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"background-color:#00008B; \n            color:white; \n            padding:12px; \n            border-radius:8px; \n            font-size:20px; \n            font-weight:bold;\">\n    8. Division of Labour\n</div>\n\nThe development of features was done by all - Johannes created character-level features; Charlotte created the sentiment features; Oliver made the word embedding features. Everyone helped in joining all of these together. Oliver worked on the overall function that included preprocessing and feature extraction. Johannes and Charlotte did some research about the topic and provided detailed descriptions of each step of the notebook. Everyone was involved in building and testing the models. Charlotte and Oliver formatted the submission part.","metadata":{}}]}